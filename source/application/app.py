from asyncio import Event, Queue, QueueEmpty, create_task, gather, sleep
from contextlib import suppress
from datetime import datetime
from re import compile
from urllib.parse import urlparse
from pathlib import Path

from aiofiles import open
from fastapi import FastAPI
from fastapi.responses import RedirectResponse

# from aiohttp import web
from pyperclip import copy, paste
from uvicorn import Config, Server

from source.expansion import (
    BrowserCookie,
    Cleaner,
    Converter,
    Namespace,
    beautify_string,
)
from source.module import (
    __VERSION__,
    ERROR,
    MASTER,
    REPOSITORY,
    ROOT,
    VERSION_BETA,
    VERSION_MAJOR,
    VERSION_MINOR,
    WARNING,
    DataRecorder,
    ExtractData,
    ExtractParams,
    IDRecorder,
    Manager,
    MapRecorder,
    logging,
    sleep_time,
)
from source.translation import _, switch_language

from ..module import Mapping
from .download import Download
from .explore import Explore
from .image import Image
from .request import Html
from .video import Video

__all__ = ["XHS"]


def data_cache(function):
    async def inner(
        self,
        data: dict,
    ):
        if self.manager.record_data:
            download = data["ä¸‹è½½åœ°å€"]
            lives = data["åŠ¨å›¾åœ°å€"]
            await function(
                self,
                data,
            )
            data["ä¸‹è½½åœ°å€"] = download
            data["åŠ¨å›¾åœ°å€"] = lives

    return inner


class XHS:
    VERSION_MAJOR = VERSION_MAJOR
    VERSION_MINOR = VERSION_MINOR
    VERSION_BETA = VERSION_BETA
    LINK = compile(r"https?://www\.xiaohongshu\.com/explore/\S+")
    SHARE = compile(r"https?://www\.xiaohongshu\.com/discovery/item/\S+")
    SHORT = compile(r"https?://xhslink\.com/\S+")
    ID = compile(r"(?:explore|item)/(\S+)?\?")
    __INSTANCE = None
    CLEANER = Cleaner()

    def __new__(cls, *args, **kwargs):
        if not cls.__INSTANCE:
            cls.__INSTANCE = super().__new__(cls)
        return cls.__INSTANCE

    def __init__(
        self,
        mapping_data: dict = None,
        work_path="",
        folder_name="Download",
        name_format="å‘å¸ƒæ—¶é—´ ä½œè€…æ˜µç§° ä½œå“æ ‡é¢˜",
        user_agent: str = None,
        cookie: str = "",
        proxy: str | dict = None,
        timeout=10,
        chunk=1024 * 1024,
        max_retry=5,
        record_data=False,
        image_format="PNG",
        image_download=True,
        video_download=True,
        live_download=False,
        folder_mode=False,
        download_record=True,
        author_archive=False,
        write_mtime=False,
        markdown_record=False,
        language="zh_CN",
        read_cookie: int | str = None,
        _print: bool = True,
        *args,
        **kwargs,
    ):
        switch_language(language)
        self.manager = Manager(
            ROOT,
            work_path,
            folder_name,
            name_format,
            chunk,
            user_agent,
            self.read_browser_cookie(read_cookie) or cookie,
            proxy,
            timeout,
            max_retry,
            record_data,
            image_format,
            image_download,
            video_download,
            live_download,
            download_record,
            folder_mode,
            author_archive,
            write_mtime,
            markdown_record,
            _print,
        )
        self.mapping_data = mapping_data or {}
        self.map_recorder = MapRecorder(
            self.manager,
        )
        self.mapping = Mapping(self.manager, self.map_recorder)
        self.html = Html(self.manager)
        self.image = Image()
        self.video = Video()
        self.explore = Explore()
        self.convert = Converter()
        self.download = Download(self.manager)
        self.id_recorder = IDRecorder(self.manager)
        self.data_recorder = DataRecorder(self.manager)
        self.clipboard_cache: str = ""
        self.queue = Queue()
        self.event = Event()
        # self.runner = self.init_server()
        # self.site = None
        self.server = None

    def __extract_image(self, container: dict, data: Namespace):
        container["ä¸‹è½½åœ°å€"], container["åŠ¨å›¾åœ°å€"] = self.image.get_image_link(
            data, self.manager.image_format
        )

    def __extract_video(self, container: dict, data: Namespace):
        container["ä¸‹è½½åœ°å€"] = self.video.get_video_link(data)
        container["åŠ¨å›¾åœ°å€"] = [
            None,
        ]

    async def __download_files(
        self,
        container: dict,
        download: bool,
        index,
        log,
        bar,
    ):
        name = self.__naming_rules(container)
        work_path = None
        if (u := container["ä¸‹è½½åœ°å€"]) and download:
            if await self.skip_download(i := container["ä½œå“ID"]):
                logging(log, _("ä½œå“ {0} å­˜åœ¨ä¸‹è½½è®°å½•ï¼Œè·³è¿‡ä¸‹è½½").format(i))
            else:
                path, result = await self.download.run(
                    u,
                    container["åŠ¨å›¾åœ°å€"],
                    index,
                    container["ä½œè€…ID"]
                    + "_"
                    + self.CLEANER.filter_name(container["ä½œè€…æ˜µç§°"]),
                    name,
                    container["ä½œå“ç±»å‹"],
                    container["æ—¶é—´æˆ³"],
                    log,
                    bar,
                )
                work_path = path
                await self.__add_record(i, result)
        elif not u:
            logging(log, _("æå–ä½œå“æ–‡ä»¶ä¸‹è½½åœ°å€å¤±è´¥"), ERROR)
        
        # ä¿å­˜Markdownè®°å½•ï¼ˆå¦‚æœå¯ç”¨ä¸”æœ‰ä¸‹è½½è·¯å¾„ï¼Œæˆ–è€…å¼ºåˆ¶ç”Ÿæˆåˆ°é»˜è®¤è·¯å¾„ï¼‰
        if work_path or self.manager.markdown_record:
            # å¦‚æœæ²¡æœ‰ä¸‹è½½è·¯å¾„ä½†å¯ç”¨äº†Markdownè®°å½•ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
            if not work_path:
                # è®¡ç®—é»˜è®¤è·¯å¾„ï¼ˆä¸ä¸‹è½½é€»è¾‘ä¿æŒä¸€è‡´ï¼‰
                nickname = container["ä½œè€…ID"] + "_" + self.CLEANER.filter_name(container["ä½œè€…æ˜µç§°"])
                from pathlib import Path
                if self.manager.author_archive:
                    folder = self.manager.folder.joinpath(nickname)
                    folder.mkdir(exist_ok=True)
                else:
                    folder = self.manager.folder
                work_path = self.manager.archive(folder, name, self.manager.folder_mode)
                work_path.mkdir(exist_ok=True)
            
            await self.save_markdown_record(container, work_path)
        
        await self.save_data(container)

    @data_cache
    async def save_data(
        self,
        data: dict,
    ):
        data["é‡‡é›†æ—¶é—´"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        data["ä¸‹è½½åœ°å€"] = " ".join(data["ä¸‹è½½åœ°å€"])
        data["åŠ¨å›¾åœ°å€"] = " ".join(i or "NaN" for i in data["åŠ¨å›¾åœ°å€"])
        data.pop("æ—¶é—´æˆ³", None)
        await self.data_recorder.add(**data)

    def generate_markdown_content(self, data: dict) -> str:
        """ç”Ÿæˆä½œå“ä¿¡æ¯çš„Markdownæ ¼å¼å†…å®¹"""
        content = f"""# {data.get('ä½œå“æ ‡é¢˜', 'æœªçŸ¥æ ‡é¢˜')}

## ğŸ“‹ ä½œå“åŸºæœ¬ä¿¡æ¯

| å­—æ®µ | å†…å®¹ |
|------|------|
| **ä½œå“ID** | {data.get('ä½œå“ID', 'æœªçŸ¥')} |
| **ä½œå“ç±»å‹** | {data.get('ä½œå“ç±»å‹', 'æœªçŸ¥')} |
| **å‘å¸ƒæ—¶é—´** | {data.get('å‘å¸ƒæ—¶é—´', 'æœªçŸ¥')} |
| **æœ€åæ›´æ–°** | {data.get('æœ€åæ›´æ–°æ—¶é—´', 'æœªçŸ¥')} |
| **ä½œå“é“¾æ¥** | [{data.get('ä½œå“é“¾æ¥', '#')}]({data.get('ä½œå“é“¾æ¥', '#')}) |

## ğŸ‘¤ ä½œè€…ä¿¡æ¯

| å­—æ®µ | å†…å®¹ |
|------|------|
| **ä½œè€…æ˜µç§°** | {data.get('ä½œè€…æ˜µç§°', 'æœªçŸ¥')} |
| **ä½œè€…ID** | {data.get('ä½œè€…ID', 'æœªçŸ¥')} |
| **ä½œè€…é“¾æ¥** | [{data.get('ä½œè€…é“¾æ¥', '#')}]({data.get('ä½œè€…é“¾æ¥', '#')}) |

## ğŸ“Š äº’åŠ¨æ•°æ®

| å­—æ®µ | æ•°é‡ |
|------|------|
| **ç‚¹èµæ•°é‡** | {data.get('ç‚¹èµæ•°é‡', '0')} |
| **æ”¶è—æ•°é‡** | {data.get('æ”¶è—æ•°é‡', '0')} |
| **è¯„è®ºæ•°é‡** | {data.get('è¯„è®ºæ•°é‡', '0')} |
| **åˆ†äº«æ•°é‡** | {data.get('åˆ†äº«æ•°é‡', '0')} |

## ğŸ“ ä½œå“æè¿°

{data.get('ä½œå“æè¿°', 'æš‚æ— æè¿°')}

## ğŸ·ï¸ ä½œå“æ ‡ç­¾

{data.get('ä½œå“æ ‡ç­¾', 'æš‚æ— æ ‡ç­¾')}

## ğŸ“¥ ä¸‹è½½ä¿¡æ¯

- **ä¸‹è½½åœ°å€æ•°é‡**: {len(data.get('ä¸‹è½½åœ°å€', '').split()) if isinstance(data.get('ä¸‹è½½åœ°å€', ''), str) and data.get('ä¸‹è½½åœ°å€') else (len(data.get('ä¸‹è½½åœ°å€', [])) if isinstance(data.get('ä¸‹è½½åœ°å€', []), list) else 0)}
- **åŠ¨å›¾åœ°å€æ•°é‡**: {len([i for i in data.get('åŠ¨å›¾åœ°å€', '').split() if i != 'NaN']) if isinstance(data.get('åŠ¨å›¾åœ°å€', ''), str) and data.get('åŠ¨å›¾åœ°å€') else (len([i for i in data.get('åŠ¨å›¾åœ°å€', []) if i and i != 'NaN']) if isinstance(data.get('åŠ¨å›¾åœ°å€', []), list) else 0)}
- **é‡‡é›†æ—¶é—´**: {data.get('é‡‡é›†æ—¶é—´', 'æœªçŸ¥')}

---

*æ­¤æ–‡ä»¶ç”± XHS-Downloader è‡ªåŠ¨ç”Ÿæˆ*
"""
        return content

    async def save_markdown_record(self, data: dict, work_path):
        """å°†ä½œå“ä¿¡æ¯ä¿å­˜ä¸ºMarkdownæ–‡ä»¶åˆ°ä½œå“æ–‡ä»¶å¤¹"""
        if not self.manager.markdown_record:
            return
        
        # æ•°æ®ç±»å‹æ£€æŸ¥
        if not isinstance(data, dict):
            logging(None, _("ç”ŸæˆMarkdownè®°å½•å¤±è´¥: æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œéœ€è¦å­—å…¸ç±»å‹"), ERROR)
            return
            
        try:
            # åˆ›å»ºä¸€ä¸ªå‰¯æœ¬æ¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            data_copy = data.copy()
            
            # ç¡®ä¿ä¸‹è½½åœ°å€å’ŒåŠ¨å›¾åœ°å€æ˜¯å­—ç¬¦ä¸²æ ¼å¼
            if isinstance(data_copy.get('ä¸‹è½½åœ°å€'), list):
                data_copy['ä¸‹è½½åœ°å€'] = " ".join(data_copy['ä¸‹è½½åœ°å€'])
            if isinstance(data_copy.get('åŠ¨å›¾åœ°å€'), list):
                data_copy['åŠ¨å›¾åœ°å€'] = " ".join(str(i) if i else "NaN" for i in data_copy['åŠ¨å›¾åœ°å€'])
            
            # ç”ŸæˆMarkdownå†…å®¹
            markdown_content = self.generate_markdown_content(data_copy)
            
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            markdown_filename = f"{data_copy.get('ä½œå“ID', 'unknown')}_info.md"
            markdown_path = work_path / markdown_filename
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            markdown_path.parent.mkdir(parents=True, exist_ok=True)
            
            # å†™å…¥æ–‡ä»¶
            async with open(markdown_path, 'w', encoding='utf-8') as f:
                await f.write(markdown_content)
            
            logging(None, _("å·²ç”Ÿæˆä½œå“Markdownè®°å½•: {0}").format(markdown_filename))
            
        except Exception as e:
            logging(None, _("ç”ŸæˆMarkdownè®°å½•å¤±è´¥: {0}").format(str(e)), ERROR)

    async def __add_record(self, id_: str, result: list) -> None:
        if all(result):
            await self.id_recorder.add(id_)

    async def extract(
        self,
        url: str,
        download=False,
        index: list | tuple = None,
        log=None,
        bar=None,
        data=True,
    ) -> list[dict]:
        # return  # è°ƒè¯•ä»£ç 
        urls = await self.extract_links(url, log)
        if not urls:
            logging(log, _("æå–å°çº¢ä¹¦ä½œå“é“¾æ¥å¤±è´¥"), WARNING)
        else:
            logging(log, _("å…± {0} ä¸ªå°çº¢ä¹¦ä½œå“å¾…å¤„ç†...").format(len(urls)))
        # return urls  # è°ƒè¯•ä»£ç 
        return [
            await self.__deal_extract(
                i,
                download,
                index,
                log,
                bar,
                data,
            )
            for i in urls
        ]

    async def extract_cli(
        self,
        url: str,
        download=True,
        index: list | tuple = None,
        log=None,
        bar=None,
        data=False,
    ) -> None:
        url = await self.extract_links(url, log)
        if not url:
            logging(log, _("æå–å°çº¢ä¹¦ä½œå“é“¾æ¥å¤±è´¥"), WARNING)
        else:
            await self.__deal_extract(
                url[0],
                download,
                index,
                log,
                bar,
                data,
            )

    async def extract_links(self, url: str, log) -> list:
        urls = []
        for i in url.split():
            if u := self.SHORT.search(i):
                i = await self.html.request_url(
                    u.group(),
                    False,
                    log,
                )
            if u := self.SHARE.search(i):
                urls.append(u.group())
            elif u := self.LINK.search(i):
                urls.append(u.group())
        return urls

    def extract_id(self, links: list[str]) -> list[str]:
        ids = []
        for i in links:
            if j := self.ID.search(i):
                ids.append(j.group(1))
        return ids

    async def __deal_extract(
        self,
        url: str,
        download: bool,
        index: list | tuple | None,
        log,
        bar,
        data: bool,
        cookie: str = None,
        proxy: str = None,
    ):
        if await self.skip_download(i := self.__extract_link_id(url)) and not data:
            msg = _("ä½œå“ {0} å­˜åœ¨ä¸‹è½½è®°å½•ï¼Œè·³è¿‡å¤„ç†").format(i)
            logging(log, msg)
            return {"message": msg}
        logging(log, _("å¼€å§‹å¤„ç†ä½œå“ï¼š{0}").format(i))
        html = await self.html.request_url(
            url,
            log=log,
            cookie=cookie,
            proxy=proxy,
        )
        namespace = self.__generate_data_object(html)
        if not namespace:
            logging(log, _("{0} è·å–æ•°æ®å¤±è´¥").format(i), ERROR)
            return {}
        data = self.explore.run(namespace)
        # logging(log, data)  # è°ƒè¯•ä»£ç 
        if not data:
            logging(log, _("{0} æå–æ•°æ®å¤±è´¥").format(i), ERROR)
            return {}
        if data["ä½œå“ç±»å‹"] == _("è§†é¢‘"):
            self.__extract_video(data, namespace)
        elif data["ä½œå“ç±»å‹"] == _("å›¾æ–‡"):
            self.__extract_image(data, namespace)
        else:
            data["ä¸‹è½½åœ°å€"] = []
        await self.update_author_nickname(data, log)
        await self.__download_files(data, download, index, log, bar)
        logging(log, _("ä½œå“å¤„ç†å®Œæˆï¼š{0}").format(i))
        await sleep_time()
        return data

    async def update_author_nickname(
        self,
        container: dict,
        log,
    ):
        if a := self.CLEANER.filter_name(
            self.mapping_data.get(i := container["ä½œè€…ID"], "")
        ):
            container["ä½œè€…æ˜µç§°"] = a
        else:
            container["ä½œè€…æ˜µç§°"] = self.manager.filter_name(container["ä½œè€…æ˜µç§°"]) or i
        await self.mapping.update_cache(
            i,
            container["ä½œè€…æ˜µç§°"],
            log,
        )

    @staticmethod
    def __extract_link_id(url: str) -> str:
        link = urlparse(url)
        return link.path.split("/")[-1]

    def __generate_data_object(self, html: str) -> Namespace:
        data = self.convert.run(html)
        return Namespace(data)

    def __naming_rules(self, data: dict) -> str:
        keys = self.manager.name_format.split()
        values = []
        for key in keys:
            match key:
                case "å‘å¸ƒæ—¶é—´":
                    values.append(self.__get_name_time(data))
                case "ä½œå“æ ‡é¢˜":
                    values.append(self.__get_name_title(data))
                case _:
                    values.append(data[key])
        return beautify_string(
            self.CLEANER.filter_name(
                self.manager.SEPARATE.join(values),
                default=self.manager.SEPARATE.join(
                    (
                        data["ä½œè€…ID"],
                        data["ä½œå“ID"],
                    )
                ),
            ),
            length=128,
        )

    @staticmethod
    def __get_name_time(data: dict) -> str:
        return data["å‘å¸ƒæ—¶é—´"].replace(":", ".")

    def __get_name_title(self, data: dict) -> str:
        return (
            beautify_string(
                self.manager.filter_name(data["ä½œå“æ ‡é¢˜"]),
                64,
            )
            or data["ä½œå“ID"]
        )

    async def monitor(
        self,
        delay=1,
        download=False,
        log=None,
        bar=None,
        data=True,
    ) -> None:
        logging(
            None,
            _(
                "ç¨‹åºä¼šè‡ªåŠ¨è¯»å–å¹¶æå–å‰ªè´´æ¿ä¸­çš„å°çº¢ä¹¦ä½œå“é“¾æ¥ï¼Œå¹¶è‡ªåŠ¨ä¸‹è½½é“¾æ¥å¯¹åº”çš„ä½œå“æ–‡ä»¶ï¼Œå¦‚éœ€å…³é—­ï¼Œè¯·ç‚¹å‡»å…³é—­æŒ‰é’®ï¼Œæˆ–è€…å‘å‰ªè´´æ¿å†™å…¥ â€œcloseâ€ æ–‡æœ¬ï¼"
            ),
            style=MASTER,
        )
        self.event.clear()
        copy("")
        await gather(
            self.__get_link(delay),
            self.__receive_link(delay, download, None, log, bar, data),
        )

    async def __get_link(self, delay: int):
        while not self.event.is_set():
            if (t := paste()).lower() == "close":
                self.stop_monitor()
            elif t != self.clipboard_cache:
                self.clipboard_cache = t
                create_task(self.__push_link(t))
            await sleep(delay)

    async def __push_link(
        self,
        content: str,
    ):
        await gather(
            *[self.queue.put(i) for i in await self.extract_links(content, None)]
        )

    async def __receive_link(self, delay: int, *args, **kwargs):
        while not self.event.is_set() or self.queue.qsize() > 0:
            with suppress(QueueEmpty):
                await self.__deal_extract(self.queue.get_nowait(), *args, **kwargs)
            await sleep(delay)

    def stop_monitor(self):
        self.event.set()

    async def skip_download(self, id_: str) -> bool:
        return bool(await self.id_recorder.select(id_))

    async def __aenter__(self):
        await self.id_recorder.__aenter__()
        await self.data_recorder.__aenter__()
        await self.map_recorder.__aenter__()
        return self

    async def __aexit__(self, exc_type, exc_value, traceback):
        await self.id_recorder.__aexit__(exc_type, exc_value, traceback)
        await self.data_recorder.__aexit__(exc_type, exc_value, traceback)
        await self.map_recorder.__aexit__(exc_type, exc_value, traceback)
        await self.close()

    async def close(self):
        await self.manager.close()

    @staticmethod
    def read_browser_cookie(value: str | int) -> str:
        return (
            BrowserCookie.get(
                value,
                domains=[
                    "xiaohongshu.com",
                ],
            )
            if value
            else ""
        )

    # @staticmethod
    # async def index(request):
    #     return web.HTTPFound(REPOSITORY)

    # async def handle(self, request):
    #     data = await request.post()
    #     url = data.get("url")
    #     download = data.get("download", False)
    #     index = data.get("index")
    #     skip = data.get("skip", False)
    #     url = await self.__extract_links(url, None)
    #     if not url:
    #         msg = _("æå–å°çº¢ä¹¦ä½œå“é“¾æ¥å¤±è´¥")
    #         data = None
    #     else:
    #         if data := await self.__deal_extract(url[0], download, index, None, None, not skip, ):
    #             msg = _("è·å–å°çº¢ä¹¦ä½œå“æ•°æ®æˆåŠŸ")
    #         else:
    #             msg = _("è·å–å°çº¢ä¹¦ä½œå“æ•°æ®å¤±è´¥")
    #             data = None
    #     return web.json_response(dict(message=msg, url=url[0], data=data))

    # def init_server(self, ):
    #     app = web.Application(debug=True)
    #     app.router.add_get('/', self.index)
    #     app.router.add_post('/xhs/', self.handle)
    #     return web.AppRunner(app)

    # async def run_server(self, log=None, ):
    #     try:
    #         await self.start_server(log)
    #         while True:
    #             await sleep(3600)  # ä¿æŒæœåŠ¡å™¨è¿è¡Œ
    #     except (CancelledError, KeyboardInterrupt):
    #         await self.close_server(log)

    # async def start_server(self, log=None, ):
    #     await self.runner.setup()
    #     self.site = web.TCPSite(self.runner, "0.0.0.0")
    #     await self.site.start()
    #     logging(log, _("Web API æœåŠ¡å™¨å·²å¯åŠ¨ï¼"))
    #     logging(log, _("æœåŠ¡å™¨ä¸»æœºåŠç«¯å£: {0}".format(self.site.name, )))

    # async def close_server(self, log=None, ):
    #     await self.runner.cleanup()
    #     logging(log, _("Web API æœåŠ¡å™¨å·²å…³é—­ï¼"))

    async def run_server(
        self,
        host="0.0.0.0",
        port=6666,
        log_level="info",
    ):
        self.server = FastAPI(
            debug=self.VERSION_BETA,
            title="XHS-Downloader",
            version=__VERSION__,
        )
        self.setup_routes()
        config = Config(
            self.server,
            host=host,
            port=port,
            log_level=log_level,
        )
        server = Server(config)
        await server.serve()

    def setup_routes(self):
        @self.server.get("/")
        async def index():
            return RedirectResponse(url=REPOSITORY)

        @self.server.post(
            "/xhs/",
            response_model=ExtractData,
        )
        async def handle(extract: ExtractParams):
            url = await self.extract_links(extract.url, None)
            if not url:
                msg = _("æå–å°çº¢ä¹¦ä½œå“é“¾æ¥å¤±è´¥")
                data = None
            else:
                if data := await self.__deal_extract(
                    url[0],
                    extract.download,
                    extract.index,
                    None,
                    None,
                    not extract.skip,
                    extract.cookie,
                    extract.proxy,
                ):
                    msg = _("è·å–å°çº¢ä¹¦ä½œå“æ•°æ®æˆåŠŸ")
                else:
                    msg = _("è·å–å°çº¢ä¹¦ä½œå“æ•°æ®å¤±è´¥")
                    data = None
            return ExtractData(message=msg, params=extract, data=data)
